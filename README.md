# xcat/spark

Spark scripts to launch a spark cluster on xcat. Optionally, the script can
install hail and its dependencies into the job environment.

## Usage

End-users envisioned usage is to use the `spark.sh` as a wrapper for their
job script. Instead of the command line this can also be used in a sbatch
script submission or interactive job. 

Usage:

```
spark.sh

Start a spark cluster within your slurm allocation.

Options:
        -i,--install-hail       Install hail in the context
        -s,--script             Executable to run within the new context (default: /bin/bash)
        --conda-env             Conda environment name to install hail into, if applicable.
        --spark-module          Name of the spark lmod module to use.
        --spark-version         Spark version to use to create the cluster (default: auto-detect).
        --hail-version          Hail version to install (default: main).
        --spark-interface       Network interface to bind for all spark daemons (default: ib0).
        --ipoib-domain          Changed domain name for infiniband network addresses.
        --master-cores          Cores to reserve for the spark master (default: 1)
        --scratch-dir           Scratch directory for temporary files generated by hail and / or Spark (default: /home/brand/scratch).
        -h,--help               Print this help message
```

Example:

```bash
sbatch --gres localtmp:15G --ntasks 30 --cpus-per-task 22 \
  --mem-per-cpu 4G --time 12:00:00 --partition batch \
  /home/brand/misc/spark/spark.sh --hail-version 0.2.89 \
  -is "/home/brand/scratch/conda/envs/xcat_spark/bin/python3 /ceph01/homedirs/brand/Projects/radarstudy/msdn_call/scripts/find_dnm/find_dnms.py -p output/spark.all/ac100 -f /ceph01/homedirs/brand/Projects/radarstudy/msdn_call/data/all.fam -R /ceph01/scratch/brand/library/human_g1k_v37_decoy.fasta --max-ac 100 INOVA:/ceph01/homedirs/brand/Projects/radarstudy/glnexus/output/inova.genome.vcf.bgz RADAR:/ceph01/homedirs/brand/Projects/radarstudy/glnexus/output/radar.grch37.final.vcf.bgz"
```

Other examples for using the script are included in the `examples/` directory.

## Dependencies

A job launched via this script depends on some Easybuild-modules, namely
`foss/2021b lz4 Rust Anaconda3 Spark/3.5.0`. By default, a conda environment
is created to install python dependencies into (default name: `spark`). The
name of the Spark module can be set on the command line, since it might differ
on various clusters, due to different module naming schemes (Options:
`devel/Spark/$vers`, `Spark/$vers`).
