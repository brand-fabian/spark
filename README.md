# xcat/spark

Spark scripts to launch a spark cluster on slurm based HPC-clusters.
Optionally, the script can install hail and its dependencies into the
job environment.

These scripts are primarily tested on the clusters xcat and Marvin
of the University of Bonn (see `examples/`).

## Usage

End-users envisioned usage is to use the `spark.sh` as a wrapper for their
job script. Instead of the command line this can also be used in a sbatch
script submission or interactive job. Any script that is launched by the Spark
wrapper can then consume SPARK-related environment variables (e.g. `SPARK_MASTER_IP`)
to connect to the cluster.

Usage:

```
./spark.sh

Start a spark cluster within your slurm allocation.

Options:
        -i,--install-hail       Install hail in the context
        -s,--script             Executable to run within the new context (default: /bin/bash)
        -X, --no-use-srun       If this flag is set, the script will be launched alongside the main spark task on the driver node, otherwise srun is used to start it.
        --install-s3-connector   Install the S3 connector libraries for hadoop and hail if this flag is set (default: false)
        --conda-env             Conda environment name to install hail into, if applicable.
        --conda-module          Conda easybuild module name (default: Miniconda3).
        --spark-module          Name of the spark lmod module to use.
        --spark-version         Spark version to use to create the cluster (default: auto-detect).
        --hail-version          Hail version to install (default: main).
        --spark-interface       Network interface to bind for all spark daemons (default: ib0).
        --ipoib-domain          Changed domain name for infiniband network addresses.
        --master-cores          Cores to reserve for the spark master (default: 1)
        --master-mem            Memory reserved for the spark master (in MB, default: 2000)
        --scratch-dir           Scratch directory for temporary files generated by hail and / or Spark (default: /home/brand/scratch).
        --conda-init            Command to run to initialize the current shell for using conda commands (default: empty).
        --keep-tmp              Keep the temporary installation and log directories of hail and spark after the job finishes (default: false).
        -h,--help               Print this help message
```

Example with code from [here](https://github.com/brand-fabian/radarstudy/blob/main/analysis/find_dnm/find_dnms.py):

```bash
sbatch \
  --gres localtmp:15G \
  --ntasks 30 \
  --cpus-per-task 22 \
  --mem-per-cpu 4G \
  --time 12:00:00 \
  --partition batch \
  /home/brand/misc/spark/spark.sh --hail-version 0.2.89 \
    -Xis "/home/brand/scratch/conda/envs/xcat_spark/bin/python3 \
      /ceph01/homedirs/brand/Projects/radarstudy/msdn_call/scripts/find_dnm/find_dnms.py \
      -p output/spark.all/ac100 \
      -f /ceph01/homedirs/brand/Projects/radarstudy/msdn_call/data/all.fam \
      -R /ceph01/scratch/brand/library/human_g1k_v37_decoy.fasta \
      --max-ac 100 \
      INOVA:/ceph01/homedirs/brand/Projects/radarstudy/glnexus/output/inova.genome.vcf.bgz \
      RADAR:/ceph01/homedirs/brand/Projects/radarstudy/glnexus/output/radar.grch37.final.vcf.bgz"
```

Other examples for using the script are included in the `examples/` directory.

## Dependencies

A job launched via this script depends on some Easybuild-modules, namely
`foss/2021b lz4 Rust Anaconda3 Spark/$vers`. By default, a conda environment
is created and used to install python dependencies into (default name: `spark`). The
name of the Spark and Conda modules can be set on the command line, since it might differ
on various clusters, due to different module naming schemes (e.g.
`devel/Spark/$vers`, `Spark/$vers`).

## Networking

Aside from the dependency management using Easybuild, this cluster assumes that a high
speed infiniband (or similar) network exists within your cluster and will try to infer
the name of the nodes on this network using the following rules (cf. start-spark.sh):
1. (Default) Replace the `eth0` part of the node name with `ib0`, if present
2. (If `--ipoib-domain` is set) Replace the domain of the node with the domain name and resolve the IP address using `dig`.
